apiVersion: inc.example.com/v1alpha1
kind: InternalTelemetryDeployment
metadata:
  name: webAppIntedDeployment
spec:
  deploymentSpec: ...
  requiredProgram: programName # when building compatibility graph controller will consider only devices with this program installed
  collector:
    selector:
      label: myCollector
    config: ...
  monitorFromInternetToPods: true
  monitorFromPodsToInternet: true
  requireAtLeastIntDevices: 50% # e.g. if we have 4 layers of switches in the tree we need at least 2 with enabled INT
  ingressInfo:
    type: node-port-on-single-node
    nodeNames:
      - w1
      - w3
    serviceName: someNodePortService


user creates this, when such deployment is created:
  - controller is notified
  - controller fetches topology
  - controller finds out paths net -> nodes, nodes -> net and looks for such that there is at least requireAtLeastIntDevices
    devices that are both P4IncEnabled and have programName installed
  - controller writes node labels where pods can be scheduled (if any) and creates deployment
  - it sets status that telemetry is not yet setup (somehow, per pod)
  - controller observes these pods, once they are scheduled it picks up the nodes and:
    - contancts Telemetry plugin for SdnShim to install telemetry for that node (on required switches), updates status that telemetry installation is in progress
    - telemetry plugin maps that request to necessary flow rules and assures that they are installed then it notifies controller which updates status 
  - when all pods have telemetry setup we are done and telemetry is running, controller should periodically query collector to find out status of devices
  - as for collector, for now assume it runs somewhere and we know where

scheduler would take cluster graph, pick feasible nodes (filter scheduler stage?), then normal scheduler will pick the best of them

OR controller would take cluster graph and set affinity prior to scheduling (better?)
can node labels be assigned to all feasible nodes, eg: canHostWebAppIntedDeployment: true, then for pods we will add nodeSelector: canHostWebAppIntedDeployment: true
but that may be inefficient, other option is to have nodes pre-labeled, eg: connectedToLeafSwitch: s1, then we would set connectedToLeafSwitch in (s_x, s_y, s_z)
problem: large cluster and a lot of feasbile nodes, order of 1000, affinity would be huge

kubectl describe intdepl webAppIntedDeployment should print:
switchA:
  - averageLatency: 123
  - averageQueueFillState: 90%
switchB: ...

then we could claim that we could have other controller that would monitor those metrics and migrate pods

also remember about multi-path situations


as for ingressInfo, to monitor traffic we need to know it's origin, the scenario that we should consider is that:
- there are some devices of type External (let's assume just 1 for simplicity), they are gateways to the cluster
- to access some internal service we must use ingress
- the flow is: user sends request to the ingress, request flies through External, DIRECTLY to the node where ingress runs, this traffic is not tunnelled in GRE
- there might be more than 1 node where ingress runs
- ingress forwards request to proper backend, this traffic is tunneled as ingress pod send it to some backend pod
- we want to monitor both cases:
  - traffic from external to ingress <- this is problematic for http ingress (p4 can't look at urls to correlate ingress with backend), we can also consider node-port ~ingress, but assure that only certain node(s) are directed, this would have a similar effect and would be doable since we could correlate raports based on ports
  - and from ingress to backend
- monitoring of external -> ingress traffic should be best-effort, our eintDepl takes ingress info as input, scheduler cares only about ingress -> pods

