apiVersion: inc.example.com/v1alpha1
kind: InternalTelemetryDeployment
metadata:
  name: webAppIntedDeployment
spec:
  deploymentSpec: ...
  requiredProgram: programName # when building compatibility graph controller will consider only devices with this program installed
  collector:
    selector:
      label: myCollector
    config: ...
  monitorFromInternetToPods: true
  monitorFromPodsToInternet: true
  requireAtLeastIntDevices: 50% # e.g. if we have 4 layers of switches in the tree we need at least 2 with enabled INT


user creates this, when such deployment is created:
  - controller is notified
  - controller fetches topology
  - controller finds out paths net -> nodes, nodes -> net and looks for such that there is at least requireAtLeastIntDevices
    devices that are both P4IncEnabled and have programName installed
  - controller writes node labels where pods can be scheduled (if any) and creates deployment
  - it sets status that telemetry is not yet setup (somehow, per pod)
  - controller observes these pods, once they are scheduled it picks up the nodes and:
    - contancts Telemetry plugin for SdnShim to install telemetry for that node (on required switches), updates status that telemetry installation is in progress
    - telemetry plugin maps that request to necessary flow rules and assures that they are installed then it notifies controller which updates status 
  - when all pods have telemetry setup we are done and telemetry is running, controller should periodically query collector to find out status of devices
  - as for collector, for now assume it runs somewhere and we know where

scheduler would take cluster graph, pick feasible nodes (filter scheduler stage?), then normal scheduler will pick the best of them

OR controller would take cluster graph and set affinity prior to scheduling (better?)
can node labels be assigned to all feasible nodes, eg: canHostWebAppIntedDeployment: true, then for pods we will add nodeSelector: canHostWebAppIntedDeployment: true
but that may be inefficient, other option is to have nodes pre-labeled, eg: connectedToLeafSwitch: s1, then we would set connectedToLeafSwitch in (s_x, s_y, s_z)
problem: large cluster and a lot of feasbile nodes, order of 1000, affinity would be huge

kubectl describe intdepl webAppIntedDeployment should print:
switchA:
  - averageLatency: 123
  - averageQueueFillState: 90%
switchB: ...

then we could claim that we could have other controller that would monitor those metrics and migrate pods

also remember about multi-path situations
